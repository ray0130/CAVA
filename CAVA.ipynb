{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **C**hain of **A**gent with chain of **V**erific**A**tion (CAVA)\n"
      ],
      "metadata": {
        "id": "yUqOfW0qDSsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "TR8XXMxGDlHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJ08UiFnDOMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c3a5a8-ffbb-4e20-98df-aba5c326f282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.0/475.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.8/411.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.7/343.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-core langchain-text-splitters langchain-community langgraph langchain_chroma langchain-huggingface langsmith\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "tVoq_pKSD5jY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LLM Model\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "rQPMgb9Yewoa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "token = os.environ[\"HUGGINGFACE_HUB_TOKEN\"]\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "hD9730wZQ2hv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "477935b3-790e-4c13-e25e-f0c6742435df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'HUGGINGFACE_HUB_TOKEN'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3672056587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUGGINGFACE_HUB_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'HUGGINGFACE_HUB_TOKEN'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = userdata.get('HUGGINGFACE_HUB_TOKEN')\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "LPv-_Sb-PRbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=32,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(llm.invoke(\"What is an LLM?\"))\n"
      ],
      "metadata": {
        "id": "5Z-QI0nGPSgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=64,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
        ")\n",
        "original_long_text = \"testing split text\" * 10\n",
        "\n",
        "chunks = splitter.split_text(original_long_text)\n",
        "chunks"
      ],
      "metadata": {
        "id": "TISheIqlPVyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split pure text\n",
        "\n",
        "# def split_text(text, chunk_size=500):\n",
        "#     chunks = []\n",
        "#     chunk_idx = 0\n",
        "#     while chunk_idx < len(text):\n",
        "#         end_idx = min(chunk_idx+chunk_size, len(text))\n",
        "#         chunks.append(text[chunk_idx:end_idx])\n",
        "#         chunk_idx = end_idx\n",
        "#     return chunks\n",
        "#     # return splitter.split_documents(text)\n",
        "\n",
        "# original_long_text = \"testing split text\" * 10\n",
        "# split_long_text = split_text(original_long_text, chunk_size=50)\n",
        "# split_long_text"
      ],
      "metadata": {
        "id": "DocimMLUkX51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts ------------------------------------------------------------------\n",
        "# For QA Tasks\n",
        "WORKER_PROMPT = lambda i, query, chunk, prev: f\"\"\"\n",
        "You are Worker {i} in a chain solving a long-context task.\n",
        "ONLY use the provided chunk and previous message.\n",
        "You need to read current source text and summary of previous source text (if any),\n",
        "and generate a summary to include them both and that best helps answer the query.\n",
        "Keep ≤ 300 tokens. If no new info, forward previous message unchanged.\n",
        "\n",
        "Query: {query}\n",
        "Current source text: CHUNK {i} (do NOT reference other chunks):\\n{chunk}\\n\n",
        "Previous source text :\\n{prev}\n",
        "\"\"\"\n",
        "\n",
        "MANAGER_PROMPT = lambda query, final_worker_json: f\"\"\"\n",
        "You are the Manager. Synthesize the final answer.\n",
        "Please keep the final answer as short as possible and do not respond with full sentences.\n",
        "Just reply with the final answer.\n",
        "The source is too long and has been summarized. You need to answer based on the summary.\n",
        "\n",
        "Query: {query}\n",
        "Final worker Summary: {final_worker_json}\n",
        "\"\"\"\n",
        "\n",
        "# ===== CoVe =====\n",
        "PLAN_VERIFICATIONS_PROMPT = lambda query, chunk, baseline_summary: f\"\"\"\n",
        "You are verifying a summary used in a long-context QA pipeline.\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Source chunk:\n",
        "{chunk}\n",
        "\n",
        "Baseline summary:\n",
        "{baseline_summary}\n",
        "\n",
        "Task:\n",
        "Generate a small list of concrete verification questions (2–4) that help check:\n",
        "- factual correctness\n",
        "- coverage of key information relevant to the question\n",
        "- absence of unsupported claims\n",
        "\n",
        "Return the questions as a numbered list.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "EXEC_VERIFICATIONS_PROMPT = lambda query, q, chunk: f\"\"\"\n",
        "You are answering a verification question using ONLY the source chunk below.\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Verification question:\n",
        "{q}\n",
        "\n",
        "Source chunk:\n",
        "{chunk}\n",
        "\n",
        "Answer the verification question concisely and only based on the source chunk.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "GEN_FINAL_RESPONSE_PROMPT = lambda query, chunk, baseline_summary, questions, answers: f\"\"\"\n",
        "You are revising a summary for a long-context QA pipeline.\n",
        "\n",
        "Original question:\n",
        "{query}\n",
        "\n",
        "Source chunk:\n",
        "{chunk}\n",
        "\n",
        "Baseline summary:\n",
        "{baseline_summary}\n",
        "\n",
        "Verification Q&A:\n",
        "{chr(10).join(f\"Q: {q}\\nA: {a}\" for q, a in zip(questions, answers))}\n",
        "\n",
        "Task:\n",
        "Write a revised summary that:\n",
        "- corrects any factual errors in the baseline summary\n",
        "- adds missing key information supported by the source chunk\n",
        "- removes unsupported or speculative claims\n",
        "- remains concise and focused on information relevant to the question\n",
        "\n",
        "Return ONLY the revised summary.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Xs8j8Qa4gXJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agent graph\n",
        "from typing import TypedDict, List\n",
        "\n",
        "# [TODO]\n",
        "class VerificationTrace(TypedDict):\n",
        "    worker_idx: int\n",
        "    baseline_summary: str\n",
        "    verification_questions: List[str]\n",
        "    verification_answers: List[str]\n",
        "    verified_summary: str\n",
        "\n",
        "\n",
        "class CoAState(TypedDict):\n",
        "    query: str\n",
        "    chunks: List[str]\n",
        "    i: int\n",
        "    worker_outputs: List[str]\n",
        "    verbose: bool\n",
        "    manager_output: str\n",
        "\n",
        "    verification_mode: str # \"none\" | \"every\" | \"every_k\"\n",
        "    verification_k: int\n",
        "    store_verification_traces: bool\n",
        "    verification_traces: List[VerificationTrace]\n"
      ],
      "metadata": {
        "id": "1cN0TvKjfuR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def worker_node(state: CoAState):\n",
        "    i = state[\"i\"]\n",
        "    chunk = state[\"chunks\"][i]\n",
        "    if i == 0:\n",
        "        prev = \"No Previous summaries\"\n",
        "    else:\n",
        "        # Get previous worker's output\n",
        "        # print(state[\"worker_outputs\"][i-1])\n",
        "        # prev = state[\"worker_outputs\"][i-1].content\n",
        "        prev = state[\"worker_outputs\"][i-1]\n",
        "    prompt = WORKER_PROMPT(i, state[\"query\"], chunk, prev)\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Worker {i} with Prompt: \\n######{prompt}\\n#######\\n\")\n",
        "    out = llm.invoke(prompt)\n",
        "    # Note new outut\n",
        "    state[\"worker_outputs\"].append(out)\n",
        "    state[\"i\"] += 1\n",
        "    if state[\"verbose\"]:\n",
        "        # print(f\"Outputs: {out.content}\\n------------------\\n\\n\")\n",
        "        print(f\"Outputs: {out}\\n------------------\\n\\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "def manager_node(state:CoAState):\n",
        "    if state[\"verbose\"]:\n",
        "        state[\"worker_outputs\"][-1]\n",
        "    # last_worker_output = state[\"worker_outputs\"][-1].content\n",
        "    last_worker_output = state[\"worker_outputs\"][-1]\n",
        "    prompt = MANAGER_PROMPT(state[\"query\"], last_worker_output)\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Manager with Prompt: \\n######{prompt}\\n#######\\n\")\n",
        "    final_answer = llm.invoke(prompt)\n",
        "    # store final summary as last output\n",
        "    state[\"manager_output\"] = final_answer\n",
        "    if state[\"verbose\"]:\n",
        "        # print(f\"Manager Final Output: \\n#############\\n{final_answer.content}\")\n",
        "        print(f\"Manager Final Output: \\n#############\\n{final_answer}\")\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "8v8BKc5rnJrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cove(query: str, chunk: str, baseline_summary: str, worker_idx: int, verbose: bool = False) -> VerificationTrace:\n",
        "    # 1. Baseline response = baseline_summary (already produced by worker)\n",
        "\n",
        "    # 2. Plan verification questions\n",
        "    plan_prompt = PLAN_VERIFICATIONS_PROMPT(query, chunk, baseline_summary)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[CoVe][Worker {worker_idx}] Plan prompt:\\n{plan_prompt}\\n\")\n",
        "\n",
        "    plan_resp = llm.invoke(plan_prompt)\n",
        "    plan_text = str(getattr(plan_resp, \"content\", plan_resp)) # Depending on the LLM wrapper, llm.invoke() may return a plain string or a message object AIMessage(..., content=\"some text\", ...)\n",
        "\n",
        "    # crude parsing: split into lines that look like questions\n",
        "    questions = [\n",
        "        line.strip(\" -0123456789.\").strip()\n",
        "        for line in plan_text.split(\"\\n\")\n",
        "        if \"?\" in line\n",
        "    ]\n",
        "    questions = [q for q in questions if q]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[CoVe][Worker {worker_idx}] Questions:\\n{questions}\\n\")\n",
        "\n",
        "    # 3. Execute verifications (factored: one call per question)\n",
        "    answers = []\n",
        "    for q in questions:\n",
        "        exec_prompt = EXEC_VERIFICATIONS_PROMPT(query, q, chunk)\n",
        "        exec_resp = llm.invoke(exec_prompt)\n",
        "        exec_text = str(getattr(exec_resp, \"content\", exec_resp)).strip() #\n",
        "        answers.append(exec_text)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[CoVe][Worker {worker_idx}] Answers:\\n{answers}\\n\")\n",
        "\n",
        "    # 4. Generate final verified summary\n",
        "    final_prompt = GEN_FINAL_RESPONSE_PROMPT(query, chunk, baseline_summary, questions, answers)\n",
        "    final_resp = llm.invoke(final_prompt)\n",
        "    final_summary = str(getattr(final_resp, \"content\", final_resp)).strip() # [TODO] add in worker node? Depending on the LLM wrapper, llm.invoke() may return a plain string or a message object AIMessage(..., content=\"some text\", ...)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[CoVe][Worker {worker_idx}] Final verified summary:\\n{final_summary}\\n\")\n",
        "\n",
        "    trace: VerificationTrace = {\n",
        "        \"worker_idx\": worker_idx,\n",
        "        \"baseline_summary\": baseline_summary,\n",
        "        \"verification_questions\": questions,\n",
        "        \"verification_answers\": answers,\n",
        "        \"verified_summary\": final_summary,\n",
        "    }\n",
        "    return trace\n",
        "\n",
        "\n",
        "def verification_node(state: CoAState, worker_idx: int):\n",
        "    query = state[\"query\"]\n",
        "    chunk = state[\"chunks\"][worker_idx]\n",
        "\n",
        "    raw_summary = state[\"worker_outputs\"][worker_idx]\n",
        "    baseline_summary = str(getattr(raw_summary, \"content\", raw_summary)) #\n",
        "\n",
        "    trace = run_cove(\n",
        "        query=query,\n",
        "        chunk=chunk,\n",
        "        baseline_summary=baseline_summary,\n",
        "        worker_idx=worker_idx,\n",
        "        verbose=state[\"verbose\"],\n",
        "    )\n",
        "\n",
        "    # replace worker summary with verified one\n",
        "    state[\"worker_outputs\"][worker_idx] = trace[\"final_verified_summary\"]\n",
        "\n",
        "    # store trace if needed\n",
        "    if state.get(\"store_verification_traces\", False): # defaults to False if the key doesn’t exist\n",
        "        state[\"verification_traces\"].append(trace)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def maybe_run_verification(state: CoAState) -> CoAState:\n",
        "    \"\"\"\n",
        "    determine if the latest generated summary needs to be verified\n",
        "\n",
        "    Param:\n",
        "    state (returned by worker_node())\n",
        "\n",
        "    Return:\n",
        "    updated state\n",
        "    \"\"\"\n",
        "    mode = state[\"verification_mode\"] # \"none\" | \"every\" | \"every_k\"\n",
        "    k = state[\"verification_k\"]\n",
        "    current_worker_idx = state[\"i\"] - 1 # cuz in worker_node() before returning state it does state[\"i\"] += 1\n",
        "\n",
        "    if mode == \"none\":\n",
        "        return state\n",
        "    if mode == \"every\":\n",
        "        return verification_node(state, current_worker_idx)\n",
        "    if mode == \"every_k\" and (current_worker_idx + 1) % k == 0:\n",
        "        return verification_node(state, current_worker_idx)\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "Z7SIQohOsuGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_coa(query, context, chunk_size=500, verbose=False):\n",
        "    # Split context\n",
        "    # chunks = split_text(context, chunk_size=chunk_size)\n",
        "    chunks = splitter.split_text(context)\n",
        "    if verbose:\n",
        "        print(\"Text Chunks: \",chunks)\n",
        "    # assert 1==2\n",
        "    # Initialize initial CoAState\n",
        "    init_state = {\n",
        "        \"query\": query,\n",
        "        \"chunks\": chunks,\n",
        "        \"i\": 0,\n",
        "        \"worker_outputs\": [],\n",
        "        \"verbose\": verbose,\n",
        "        \"manager_output\": \"\",\n",
        "        # [CoVe]\n",
        "        \"verification_mode\": verification_mode,\n",
        "        \"verification_k\": verification_k,\n",
        "        \"store_verification_traces\": store_verification_traces,\n",
        "        \"verification_traces\": []\n",
        "    }\n",
        "    state = init_state\n",
        "    # Worker nodes, for each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Run worker node and get new state\n",
        "        state = worker_node(state)\n",
        "        # [CoVe]\n",
        "        state = maybe_run_verification(state)\n",
        "\n",
        "    # At the end of the loop, state[\"i\"] should be == len(chunks)\n",
        "    assert state[\"i\"] == len(chunks), \"Total states worked does not equal to number of text chunks\"\n",
        "\n",
        "    # Finally run manager at last\n",
        "    state = manager_node(state)\n",
        "    # final_ans = state[\"worker_outputs\"][-1].content\n",
        "    final_ans = state[\"manager_output\"]\n",
        "    if verbose:\n",
        "        print(f\"Query: {state[\"query\"]}\\nFinal Answer from Manager: {final_ans}\")\n",
        "    return final_ans"
      ],
      "metadata": {
        "id": "9HRK_kL7mXbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test run CoA\n",
        "ans = run_coa(\"what is the meaning?\", original_long_text, chunk_size=50, verbose=False)\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YtaRWUmswE7",
        "outputId": "6848f3b8-754a-48be-bdba-b4615d3d86c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='No specific semantic meaning.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019af896-5104-7aa2-acf6-9f240545e7f8-0', usage_metadata={'input_tokens': 280, 'output_tokens': 58, 'total_tokens': 338, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 53}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnMlBW_QtXZi",
        "outputId": "979333f8-f6d9-4175-dfb5-bca9ef8594cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='No specific semantic meaning.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019af896-5104-7aa2-acf6-9f240545e7f8-0', usage_metadata={'input_tokens': 280, 'output_tokens': 58, 'total_tokens': 338, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 53}})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llPlkzBMjiOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jgh-IA0MwhNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Callable\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. HotpotQA Loader\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def load_hotpotqa(split=\"validation\", max_samples=None):\n",
        "    \"\"\"\n",
        "    [source] https://huggingface.co/datasets/hotpotqa/hotpot_qa\n",
        "\n",
        "    an example in hotpotqa - fullwiki:\n",
        "    {\n",
        "        \"id\": str,\n",
        "        \"question\": str,\n",
        "        \"answer\": str,\n",
        "        \"type\": str,\n",
        "        \"level\": str,\n",
        "        \"supporting_facts\":\n",
        "        {\n",
        "            \"title\": [str, str, ...], # may repeat\n",
        "            \"sent_id\": [int32, int32, ...]\n",
        "        },\n",
        "        \"context\":\n",
        "        {\n",
        "            \"title\": [str, str, ...],\n",
        "            \"sentences\": [[str, str, str, ...], [str, str, str, ...], ...]\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    Return:\n",
        "    a list of dicts\n",
        "    {\n",
        "        \"context\":\n",
        "        [\n",
        "            { \"title\": str, \"sentences\": [str, str, ...] }, # doc 0\n",
        "            { \"title\": str, \"sentences\": [str, str, ...] }, # doc 1\n",
        "            ...\n",
        "        ]\n",
        "        \"question\": str,\n",
        "        \"answer\": str\n",
        "    }\n",
        "    \"\"\"\n",
        "    raw = load_dataset(\"hotpot_qa\", \"fullwiki\")[split]\n",
        "\n",
        "    data = []\n",
        "    for item in raw:\n",
        "        context = [\n",
        "            {\n",
        "                \"title\": t,\n",
        "                \"sentences\": sents\n",
        "            }\n",
        "            for t, sents in zip(item[\"context\"][\"title\"], item[\"context\"][\"sentences\"])\n",
        "        ]\n",
        "        question = item[\"question\"]\n",
        "        answer = item[\"answer\"]\n",
        "\n",
        "        data.append({\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer\n",
        "        })\n",
        "\n",
        "        if max_samples and len(data) >= max_samples:\n",
        "            break\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. Context Merger\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def merge_context_fullwiki(context):\n",
        "    \"\"\"\n",
        "    merge each document's sentence list into a single text string\n",
        "\n",
        "    Param:\n",
        "    context (return from load_hotpotqa()):\n",
        "    [\n",
        "        { \"title\": str, \"sentences\": [str, str, ...] }, # doc 0\n",
        "        { \"title\": str, \"sentences\": [str, str, ...] }, # doc 1\n",
        "        ...\n",
        "    ]\n",
        "\n",
        "    Return:\n",
        "    texts: list[str]\n",
        "    merged text for each document\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "\n",
        "    for doc in context:\n",
        "        text = \" \".join(doc[\"sentences\"])\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. Evaluation Metrics (EM + F1)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Lowercase, remove punctuation/articles/extra whitespace.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "# TODO: does the paper compute this at the token level??\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_answer(pred).split()\n",
        "    gold_tokens = normalize_answer(gold).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return int(pred_tokens == gold_tokens)\n",
        "\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = sum(min(pred_tokens.count(t), gold_tokens.count(t)) for t in common)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return normalize_answer(pred) == normalize_answer(gold)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. Evaluation loop\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def evaluate(model_fn: Callable, dataset: List[Dict], chunk_size=500):\n",
        "    \"\"\"\n",
        "    model_fn(query, context_chunks) -> str\n",
        "    \"\"\"\n",
        "    qs = []\n",
        "    ctxs = []\n",
        "    preds = []\n",
        "    refs = []\n",
        "    f1s = []\n",
        "    ems = []\n",
        "\n",
        "    for sample in tqdm(dataset, desc=\"Evaluating\"):\n",
        "        question = sample[\"question\"]\n",
        "        context = sample[\"context\"]\n",
        "        gold = sample[\"answer\"]\n",
        "\n",
        "        texts = merge_context_fullwiki(context)\n",
        "\n",
        "        pred = model_fn(question, texts)\n",
        "\n",
        "        qs.append(question)\n",
        "        ctxs.append(texts)\n",
        "        preds.append(pred)\n",
        "        refs.append(gold)\n",
        "\n",
        "        f1s.append(f1_score(pred, gold))\n",
        "        ems.append(int(exact_match(pred, gold)))\n",
        "\n",
        "    return {\n",
        "        \"contexts\": ctxs,\n",
        "        \"questions\": qs,\n",
        "        \"predictions\": preds,\n",
        "        \"references\": refs,\n",
        "        \"f1\": sum(f1s) / len(f1s),\n",
        "        \"em\": sum(ems) / len(ems)\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 5. Placeholder CoA model\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def coa_placeholder(question: str, context_texts: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Dummy version to make the pipeline runnable now.\n",
        "    Replace with Ray's CoA later.\n",
        "    \"\"\"\n",
        "    merged_context = \" \".join(context_texts)\n",
        "    print(merged_context)\n",
        "    # prompt = f\"Context:\\n{merged_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    # TODO\n",
        "    # print(f\"Length of merged context: {len(merged_context)}\")\n",
        "    # return \"hello\"\n",
        "    # assert 1==2\n",
        "    # TODO: Figure out what chunk size is best cost to performance\n",
        "    final_ans = run_coa(query=question, context=merged_context, chunk_size=2000)\n",
        "    return final_ans\n",
        "    return \"PLACEHOLDER YES\" # \"PLACEHOLDER_ANSWER\"\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 6. Running the pipeline\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Full Hotpot QA size is 7405, takes ~ 29 hours with 512 chunk size\n",
        "    data = load_hotpotqa(split=\"validation\", max_samples=1)\n",
        "    results = evaluate(coa_placeholder, data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIecN2liwiC8",
        "outputId": "f532c3aa-2ea5-405c-aabb-101235937939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.  He also studied cinema at the University of Southern California from 1991 to 1997.  Collis first work was the assistant director for the Scott Derrickson's short \"Love in the Ruins\" (1995).  In 1998, he played \"Crankshaft\" in Eric Koyanagi's \"Hundred Percent\". Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast. Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"  He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.  With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.  In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\". Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.  It is the fourteenth film of the Marvel Cinematic Universe (MCU).  The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.  In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident. Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film.  It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD.  It was directed by Scott Derrickson and released on October 3, 2000.  The film concerns a corrupt detective who discovers Lemarchand's box at a crime scene.  The film's reviews were mixed. Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.  It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger. Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".  The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014. Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its population was 403 at the 2010 census.  It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area.  Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.  Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr. Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.  He moved to Hollywood, California in 1948 to pursue a career in acting.  He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"  He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.  He also has since gone on to write, produce and direct several films. The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson.  The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1/1 [04:29<00:00, 269.93s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmjoTw1zuSmO",
        "outputId": "f25080a6-e428-4256-e36e-d321f5aef3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'contexts': [['Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.  He also studied cinema at the University of Southern California from 1991 to 1997.  Collis first work was the assistant director for the Scott Derrickson\\'s short \"Love in the Ruins\" (1995).  In 1998, he played \"Crankshaft\" in Eric Koyanagi\\'s \"Hundred Percent\".',\n",
              "   \"Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.  The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.  Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\",\n",
              "   'Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.  Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"  He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.  With Gunn, he has scored every one of the director\\'s films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.  In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".',\n",
              "   'Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.  It is the fourteenth film of the Marvel Cinematic Universe (MCU).  The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.  In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.',\n",
              "   'Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film.  It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD.  It was directed by Scott Derrickson and released on October 3, 2000.  The film concerns a corrupt detective who discovers Lemarchand\\'s box at a crime scene.  The film\\'s reviews were mixed.',\n",
              "   'Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.  It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.',\n",
              "   'Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.  The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".  The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.',\n",
              "   'Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its population was 403 at the 2010 census.  It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area.  Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.  Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.',\n",
              "   'Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.  He moved to Hollywood, California in 1948 to pursue a career in acting.  He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"  He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.  He also has since gone on to write, produce and direct several films.',\n",
              "   'The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson.  The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.']],\n",
              " 'questions': ['Were Scott Derrickson and Ed Wood of the same nationality?'],\n",
              " 'predictions': ['Scott Derrickson is known for his work in mainstream Hollywood, particularly in the horror genre. Their nationalities differ; Ed Wood was American, while Scott Derrickson'],\n",
              " 'references': ['yes'],\n",
              " 'f1': 0.0,\n",
              " 'em': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['f1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuZJx7NZwt-l",
        "outputId": "f79b681c-5e8c-4100-d29b-6d72c02c7d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03885281385281385"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(\":)\")"
      ],
      "metadata": {
        "id": "5brzMePh8fsM",
        "outputId": "d5c8fd95-92c4-4b8f-bfb7-4633de69a0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":)\n",
            ":)\n",
            ":)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(f\"[{i}] Questions:{results['questions'][i]}\\nPrediction: {results['predictions'][i]}  ---> Reference: {results['references'][i]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9T9mve3If2A",
        "outputId": "09195396-93ca-480f-e876-9a282b871704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Questions:Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "Prediction: to Scott Derrickson. Derrickson was born in 1972 and is an American filmmaker.\n",
            "No, they were not of the same nationality. Scott  ---> Reference: yes\n",
            "\n",
            "\n",
            "[1] Questions:What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\n",
            "Prediction: The answer is: Kate Beckinsale\n",
            "No government position mentioned. The answer is: None Final answer: None\n",
            "\n",
            "What government position was held by the woman  ---> Reference: Chief of Protocol\n",
            "\n",
            "\n",
            "[2] Questions:What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\n",
            "Prediction: science fantasy young adult series meeting all criteria mentioned.\n",
            "The source is too long and has been summarized. You need to answer based on the summary.\n",
            "\n",
            "Query: What  ---> Reference: Animorphs\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['contexts'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gbgG92qxiGi",
        "outputId": "6bc93edf-3888-4c0d-bfed-1a1f0aef2d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adam Collis is an American filmmaker and actor. He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010. He also studied cinema at the University of Southern California from 1991 to 1997. Collis first work was the assistant director for the Scott Derrickson\\'s short \"Love in the Ruins\" (1995). In 1998, he played \"Crankshaft\" in Eric Koyanagi\\'s \"Hundred Percent\".',\n",
              " \"Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood. The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau. Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\",\n",
              " 'Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games. Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\" He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn. With Gunn, he has scored every one of the director\\'s films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel. In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".',\n",
              " 'Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures. It is the fourteenth film of the Marvel Cinematic Universe (MCU). The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton. In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.',\n",
              " 'Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film. It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD. It was directed by Scott Derrickson and released on October 3, 2000. The film concerns a corrupt detective who discovers Lemarchand\\'s box at a crime scene. The film\\'s reviews were mixed.',\n",
              " 'Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill. It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.',\n",
              " 'Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer. The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\". The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.',\n",
              " 'Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States. Its population was 403 at the 2010 census. It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area. Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century. Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.',\n",
              " 'Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor. He moved to Hollywood, California in 1948 to pursue a career in acting. He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\" He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor. He also has since gone on to write, produce and direct several films.',\n",
              " 'The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson. The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['questions'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-JYv67Zdx6Pz",
        "outputId": "094cad69-1744-45e2-d64f-a8275289ab61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Were Scott Derrickson and Ed Wood of the same nationality?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['references'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "013xNxLfx-On",
        "outputId": "13b212d4-8a40-4ef7-ca39-8c6d7cf97c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['predictions'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bvGm1nY5yRd7",
        "outputId": "45a826a3-937a-426b-9923-4100c0df0b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to Scott Derrickson. Derrickson was born in 1972 and is an American filmmaker.\\nNo, they were not of the same nationality. Scott'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing committing from google colab"
      ],
      "metadata": {
        "id": "Ls8r2UCByW3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}