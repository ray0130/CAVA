{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **C**hain of **A**gent with chain of **V**erific**A**tion (CAVA)\n"
      ],
      "metadata": {
        "id": "yUqOfW0qDSsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "TR8XXMxGDlHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ08UiFnDOMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1d0c07-f035-4d15-f288-6129bcaf0ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-core langchain-text-splitters langchain-community langgraph langchain_chroma langchain-huggingface langsmith\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "tVoq_pKSD5jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LLM Model\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "rQPMgb9Yewoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split pure text\n",
        "\n",
        "def split_text(text, chunk_size=500):\n",
        "    chunks = []\n",
        "    chunk_idx = 0\n",
        "    while chunk_idx < len(text):\n",
        "        end_idx = min(chunk_idx+chunk_size, len(text))\n",
        "        chunks.append(text[chunk_idx:end_idx])\n",
        "        chunk_idx = end_idx\n",
        "    return chunks\n",
        "    # return splitter.split_documents(text)\n",
        "\n",
        "original_long_text = \"testing split text\" * 10\n",
        "split_long_text = split_text(original_long_text, chunk_size=50)\n",
        "split_long_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DocimMLUkX51",
        "outputId": "f8f83b67-3338-491f-fcbd-cea90756cb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testing split texttesting split texttesting split ',\n",
              " 'texttesting split texttesting split texttesting sp',\n",
              " 'lit texttesting split texttesting split texttestin',\n",
              " 'g split texttesting split text']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts ------------------------------------------------------------------\n",
        "# For QA Tasks\n",
        "WORKER_PROMPT = lambda i, query, chunk, prev: f\"\"\"\n",
        "You are Worker {i} in a chain solving a long-context task.\n",
        "ONLY use the provided chunk and previous message.\n",
        "You need to read current source text and summary of previous source text (if any),\n",
        "and generate a summary to include them both and that best helps answer the query.\n",
        "Keep ≤ 300 tokens. If no new info, forward previous message unchanged.\n",
        "\n",
        "Query: {query}\n",
        "Current source text: CHUNK {i} (do NOT reference other chunks):\\n{chunk}\\n\n",
        "Previous source text :\\n{prev}\n",
        "\"\"\"\n",
        "\n",
        "MANAGER_PROMPT = lambda query, final_worker_json: f\"\"\"\n",
        "You are the Manager. Synthesize the final answer.\n",
        "Please keep the final answer as short as possible and do not respond with full sentences.\n",
        "Just reply with the final answer.\n",
        "The source is too long and has been summarized. You need to answer based on the summary.\n",
        "\n",
        "Query: {query}\n",
        "Final worker Summary: {final_worker_json}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Xs8j8Qa4gXJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define agent graph\n",
        "from typing import TypedDict, List\n",
        "\n",
        "class CoAState(TypedDict):\n",
        "    query: str\n",
        "    chunks: List[str]\n",
        "    i: int\n",
        "    worker_outputs: List[str]\n",
        "    verbose: bool\n"
      ],
      "metadata": {
        "id": "1cN0TvKjfuR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def worker_node(state: CoAState):\n",
        "    i = state[\"i\"]\n",
        "    chunk = state[\"chunks\"][i]\n",
        "    if i == 0:\n",
        "        prev = \"No Previous summaries\"\n",
        "    else:\n",
        "        # Get previous worker's output\n",
        "        prev = state[\"worker_outputs\"][i-1].content\n",
        "    prompt = WORKER_PROMPT(i, state[\"query\"], chunk, prev)\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Worker {i} with Prompt: \\n######{prompt}\\n#######\\n\")\n",
        "    out = llm.invoke(prompt)\n",
        "    # Note new outut\n",
        "    state[\"worker_outputs\"].append(out)\n",
        "    state[\"i\"] += 1\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Outputs: {out.content}\\n------------------\\n\\n\")\n",
        "    return state\n",
        "\n",
        "def manager_node(state:CoAState):\n",
        "    last_worker_output = state[\"worker_outputs\"][-1].content\n",
        "    prompt = MANAGER_PROMPT(state[\"query\"], last_worker_output)\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Manager with Prompt: \\n######{prompt}\\n#######\\n\")\n",
        "    final_answer = llm.invoke(prompt)\n",
        "    # store final summary as last output\n",
        "    state[\"worker_outputs\"].append(final_answer)\n",
        "    if state[\"verbose\"]:\n",
        "        print(f\"Manager Final Output: \\n#############\\n{final_answer.content}\")\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "8v8BKc5rnJrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_coa(query, context, chunk_size=500, verbose=False):\n",
        "    # Split context\n",
        "    chunks = split_text(context, chunk_size=chunk_size)\n",
        "    if verbose:\n",
        "        print(\"Text Chunks: \",chunks)\n",
        "    # assert 1==2\n",
        "    # Initialize initial CoAState\n",
        "    init_state = {\n",
        "        \"query\": query,\n",
        "        \"chunks\": chunks,\n",
        "        \"i\": 0,\n",
        "        \"worker_outputs\": [],\n",
        "        \"verbose\": verbose\n",
        "    }\n",
        "    state = init_state\n",
        "    # Worker nodes, for each chunk\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Run worker node and get new state\n",
        "        state = worker_node(state)\n",
        "    # At the end of the loop, state[\"i\"] should be == len(chunks)\n",
        "    assert state[\"i\"] == len(chunks), \"Total states worked does not equal to number of text chunks\"\n",
        "\n",
        "    # Finally run manager at last\n",
        "    state = manager_node(state)\n",
        "    final_ans = state[\"worker_outputs\"][-1].content\n",
        "    if verbose:\n",
        "        print(f\"Query: {state[\"query\"]}\\nFinal Answer from Manager: {final_ans}\")\n",
        "    return final_ans"
      ],
      "metadata": {
        "id": "9HRK_kL7mXbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test run CoA\n",
        "ans = run_coa(\"what is the meaning?\", original_long_text, chunk_size=50)\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0YtaRWUmswE7",
        "outputId": "b89f7301-c39a-4697-dcef-18f5ceedceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the summary, the text does not convey any inherent semantic meaning. Instead, it consistently appears to be a functional sample or placeholder, primarily serving testing purposes related to text splitting or similar text processing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KnMlBW_QtXZi",
        "outputId": "47e7d278-cb3f-405a-dfca-62ad3dda1076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The meaning is that the text does not contain substantive content. It is identified as system test data, fragmented input, or a placeholder, devoid of any real information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llPlkzBMjiOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "jgh-IA0MwhNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Callable\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. HotpotQA Loader\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def load_hotpotqa(split=\"validation\", max_samples=None):\n",
        "    \"\"\"\n",
        "    [source] https://huggingface.co/datasets/hotpotqa/hotpot_qa\n",
        "\n",
        "    an example in hotpotqa - fullwiki:\n",
        "    {\n",
        "        \"id\": str,\n",
        "        \"question\": str,\n",
        "        \"answer\": str,\n",
        "        \"type\": str,\n",
        "        \"level\": str,\n",
        "        \"supporting_facts\":\n",
        "        {\n",
        "            \"title\": [str, str, ...], # may repeat\n",
        "            \"sent_id\": [int32, int32, ...]\n",
        "        },\n",
        "        \"context\":\n",
        "        {\n",
        "            \"title\": [str, str, ...],\n",
        "            \"sentences\": [[str, str, str, ...], [str, str, str, ...], ...]\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    Return:\n",
        "    a list of dicts\n",
        "    {\n",
        "        \"context\":\n",
        "        [\n",
        "            { \"title\": str, \"sentences\": [str, str, ...] }, # doc 0\n",
        "            { \"title\": str, \"sentences\": [str, str, ...] }, # doc 1\n",
        "            ...\n",
        "        ]\n",
        "        \"question\": str,\n",
        "        \"answer\": str\n",
        "    }\n",
        "    \"\"\"\n",
        "    raw = load_dataset(\"hotpot_qa\", \"fullwiki\")[split]\n",
        "\n",
        "    data = []\n",
        "    for item in raw:\n",
        "        context = [\n",
        "            {\n",
        "                \"title\": t,\n",
        "                \"sentences\": sents\n",
        "            }\n",
        "            for t, sents in zip(item[\"context\"][\"title\"], item[\"context\"][\"sentences\"])\n",
        "        ]\n",
        "        question = item[\"question\"]\n",
        "        answer = item[\"answer\"]\n",
        "\n",
        "        data.append({\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer\n",
        "        })\n",
        "\n",
        "        if max_samples and len(data) >= max_samples:\n",
        "            break\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. Context Chunker\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def chunk_context_fullwiki(context, chunk_size=500):\n",
        "    \"\"\"\n",
        "    split sentences of a doc into chuncks of length chunk_size\n",
        "\n",
        "    Param:\n",
        "    context (return from load_hotpotqa()):\n",
        "    [\n",
        "        { \"title\": str, \"sentences\": [str, str, ...] }, # doc 0\n",
        "        { \"title\": str, \"sentences\": [str, str, ...] }, # doc 1\n",
        "        ...\n",
        "    ]\n",
        "\n",
        "    Return:\n",
        "    a list of text chunks (str).\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for doc in context:\n",
        "        text = \" \".join(doc[\"sentences\"])\n",
        "        words = text.split()\n",
        "\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i+chunk_size])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. Evaluation Metrics (EM + F1)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Lowercase, remove punctuation/articles/extra whitespace.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "# TODO: does the paper compute this at the token level??\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_answer(pred).split()\n",
        "    gold_tokens = normalize_answer(gold).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return int(pred_tokens == gold_tokens)\n",
        "\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = sum(min(pred_tokens.count(t), gold_tokens.count(t)) for t in common)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return normalize_answer(pred) == normalize_answer(gold)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. Evaluation loop\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def evaluate(model_fn: Callable, dataset: List[Dict], chunk_size=500):\n",
        "    \"\"\"\n",
        "    model_fn(query, context_chunks) -> str\n",
        "    \"\"\"\n",
        "    qs = []\n",
        "    ctxs = []\n",
        "    preds = []\n",
        "    refs = []\n",
        "    f1s = []\n",
        "    ems = []\n",
        "\n",
        "    for sample in tqdm(dataset, desc=\"Evaluating\"):\n",
        "        question = sample[\"question\"]\n",
        "        context = sample[\"context\"]\n",
        "        gold = sample[\"answer\"]\n",
        "\n",
        "        chunks = chunk_context_fullwiki(context, chunk_size)\n",
        "\n",
        "        pred = model_fn(question, chunks) # TODO\n",
        "\n",
        "        qs.append(question)\n",
        "        ctxs.append(chunks)\n",
        "        preds.append(pred)\n",
        "        refs.append(gold)\n",
        "\n",
        "        f1s.append(f1_score(pred, gold))\n",
        "        ems.append(int(exact_match(pred, gold)))\n",
        "\n",
        "    return {\n",
        "        \"contexts\": ctxs,\n",
        "        \"questions\": qs,\n",
        "        \"predictions\": preds,\n",
        "        \"references\": refs,\n",
        "        \"f1\": sum(f1s) / len(f1s),\n",
        "        \"em\": sum(ems) / len(ems)\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 5. Placeholder CoA model\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def coa_placeholder(question: str, context_chunks: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Dummy version to make the pipeline runnable now.\n",
        "    Replace with Ray's CoA later.\n",
        "    \"\"\"\n",
        "    merged_context = \" \".join(context_chunks)\n",
        "    prompt = f\"Context:\\n{merged_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    # TODO\n",
        "    # print(f\"Length of merged context: {len(merged_context)}\")\n",
        "    # return \"hello\"\n",
        "    # assert 1==2\n",
        "    # TODO: Figure out what chunk size is best cost to performance\n",
        "    final_ans = run_coa(query=question, context=merged_context, chunk_size=2000)\n",
        "    return final_ans\n",
        "    return \"PLACEHOLDER YES\" # \"PLACEHOLDER_ANSWER\"\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 6. Running the pipeline\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = load_hotpotqa(split=\"validation\", max_samples=3)\n",
        "    results = evaluate(coa_placeholder, data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIecN2liwiC8",
        "outputId": "c9fafdc6-0282-4002-d404-a439c8756cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  67%|██████▋   | 2/3 [00:34<00:16, 16.41s/it]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
            "Please retry in 51.327240617s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 51\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
            "Please retry in 49.258330556s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 49\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
            "Please retry in 45.181607806s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 45\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
            "Please retry in 37.105264717s. [links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 37\n",
            "}\n",
            "].\n",
            "Evaluating: 100%|██████████| 3/3 [01:18<00:00, 26.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['f1']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuZJx7NZwt-l",
        "outputId": "b67794f5-66c8-4321-d27a-6cd359365c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4444444444444445"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(\":)\")"
      ],
      "metadata": {
        "id": "5brzMePh8fsM",
        "outputId": "b516e256-f993-4477-aa8f-e3721ba7e0c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":)\n",
            ":)\n",
            ":)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(f\"[{i}] Questions:{results['questions'][i]}\\nPrediction: {results['predictions'][i]}  ---> Reference: {results['references'][i]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9T9mve3If2A",
        "outputId": "05d3749f-21f9-42d6-e83c-c4e0395ebbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Questions:Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "Prediction: Yes.  ---> Reference: yes\n",
            "\n",
            "\n",
            "[1] Questions:What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\n",
            "Prediction: No information available.  ---> Reference: Chief of Protocol\n",
            "\n",
            "\n",
            "[2] Questions:What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\n",
            "Prediction: Animorphs; companion books not mentioned.  ---> Reference: Animorphs\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['contexts'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gbgG92qxiGi",
        "outputId": "0fadf49f-97f7-4138-ef24-ec0210e2119f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Adam Collis is an American filmmaker and actor. He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010. He also studied cinema at the University of Southern California from 1991 to 1997. Collis first work was the assistant director for the Scott Derrickson\\'s short \"Love in the Ruins\" (1995). In 1998, he played \"Crankshaft\" in Eric Koyanagi\\'s \"Hundred Percent\".',\n",
              " \"Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood. The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau. Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\",\n",
              " 'Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games. Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\" He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn. With Gunn, he has scored every one of the director\\'s films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel. In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".',\n",
              " 'Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures. It is the fourteenth film of the Marvel Cinematic Universe (MCU). The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton. In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.',\n",
              " 'Hellraiser: Inferno (also known as Hellraiser V: Inferno) is a 2000 American horror film. It is the fifth installment in the \"Hellraiser\" series and the first \"Hellraiser\" film to go straight-to-DVD. It was directed by Scott Derrickson and released on October 3, 2000. The film concerns a corrupt detective who discovers Lemarchand\\'s box at a crime scene. The film\\'s reviews were mixed.',\n",
              " 'Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill. It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.',\n",
              " 'Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer. The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\". The film stars Eric Bana, Édgar Ramírez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.',\n",
              " 'Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States. Its population was 403 at the 2010 census. It is part of the Little Rock–North Little Rock–Conway Metropolitan Statistical Area. Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century. Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.',\n",
              " 'Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor. He moved to Hollywood, California in 1948 to pursue a career in acting. He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\" He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor. He also has since gone on to write, produce and direct several films.',\n",
              " 'The Exorcism of Emily Rose is a 2005 American legal drama horror film directed by Scott Derrickson and starring Laura Linney and Tom Wilkinson. The film is loosely based on the story of Anneliese Michel and follows a self-proclaimed agnostic who acts as defense counsel (Linney) representing a parish priest (Wilkinson), accused by the state of negligent homicide after he performed an exorcism.']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['questions'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-JYv67Zdx6Pz",
        "outputId": "7f70be55-10cb-4a41-cb0b-81b564356829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Were Scott Derrickson and Ed Wood of the same nationality?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['references'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "013xNxLfx-On",
        "outputId": "474096fe-3031-4a1f-aaea-15e3e7c6fb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['predictions'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bvGm1nY5yRd7",
        "outputId": "60817cdf-95f1-40b2-ca2c-44056661cb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing committing from google colab"
      ],
      "metadata": {
        "id": "Ls8r2UCByW3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}